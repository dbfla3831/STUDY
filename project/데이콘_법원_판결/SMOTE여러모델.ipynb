{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca94ce9d",
   "metadata": {},
   "source": [
    "# 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77213fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:04:32.453532Z",
     "start_time": "2024-03-17T05:04:32.450842Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install -U imbalanced-learn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed4f45d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:39:49.341274Z",
     "start_time": "2024-03-17T05:39:49.339045Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "814f548f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:44:06.657174Z",
     "start_time": "2024-03-17T05:44:06.653012Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer # 표준 토큰화\n",
    "from nltk.corpus import stopwords # 불용어 제거\n",
    "from nltk.stem import WordNetLemmatizer # 기본 형태로 변환\n",
    "from imblearn.over_sampling import SMOTE # 비대칭이라 사용\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f21329",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb5a900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:39:55.527126Z",
     "start_time": "2024-03-17T05:39:55.480700Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "submission_df = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "933ebd82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:39:55.540555Z",
     "start_time": "2024-03-17T05:39:55.528142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID         first_party                    second_party  \\\n",
       "0  TRAIN_0000   Phil A. St. Amant              Herman A. Thompson   \n",
       "1  TRAIN_0001      Stephen Duncan                  Lawrence Owens   \n",
       "2  TRAIN_0002   Billy Joe Magwood  Tony Patterson, Warden, et al.   \n",
       "3  TRAIN_0003          Linkletter                          Walker   \n",
       "4  TRAIN_0004  William Earl Fikes                         Alabama   \n",
       "\n",
       "                                               facts  first_party_winner  \n",
       "0  On June 27, 1962, Phil St. Amant, a candidate ...                   1  \n",
       "1  Ramon Nelson was riding his bike when he suffe...                   0  \n",
       "2  An Alabama state court convicted Billy Joe Mag...                   1  \n",
       "3  Victor Linkletter was convicted in state court...                   0  \n",
       "4  On April 24, 1953 in Selma, Alabama, an intrud...                   1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>Salerno</td>\n",
       "      <td>United States</td>\n",
       "      <td>The 1984 Bail Reform Act allowed the federal c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>Milberg Weiss Bershad Hynes and Lerach</td>\n",
       "      <td>Lexecon, Inc.</td>\n",
       "      <td>Lexecon Inc. was a defendant in a class action...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>No. 07-582\\t Title: \\t Federal Communications ...</td>\n",
       "      <td>Fox Television Stations, Inc., et al.</td>\n",
       "      <td>In 2002 and 2003, Fox Television Stations broa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>Harold Kaufman</td>\n",
       "      <td>United States</td>\n",
       "      <td>During his trial for armed robbery of a federa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>Berger</td>\n",
       "      <td>Hanlon</td>\n",
       "      <td>In 1993, a magistrate judge issued a warrant a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                        first_party  \\\n",
       "0  TEST_0000                                            Salerno   \n",
       "1  TEST_0001             Milberg Weiss Bershad Hynes and Lerach   \n",
       "2  TEST_0002  No. 07-582\\t Title: \\t Federal Communications ...   \n",
       "3  TEST_0003                                    Harold Kaufman    \n",
       "4  TEST_0004                                             Berger   \n",
       "\n",
       "                            second_party  \\\n",
       "0                          United States   \n",
       "1                          Lexecon, Inc.   \n",
       "2  Fox Television Stations, Inc., et al.   \n",
       "3                          United States   \n",
       "4                                 Hanlon   \n",
       "\n",
       "                                               facts  \n",
       "0  The 1984 Bail Reform Act allowed the federal c...  \n",
       "1  Lexecon Inc. was a defendant in a class action...  \n",
       "2  In 2002 and 2003, Fox Television Stations broa...  \n",
       "3  During his trial for armed robbery of a federa...  \n",
       "4  In 1993, a magistrate judge issued a warrant a...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c0171c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:39:55.545568Z",
     "start_time": "2024-03-17T05:39:55.540555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "0     829\n",
       "1    1649\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('first_party_winner').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce13b7f",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff2411c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:39:55.551236Z",
     "start_time": "2024-03-17T05:39:55.546536Z"
    }
   },
   "outputs": [],
   "source": [
    "# 문자 처리\n",
    "cat_cols = ['first_party', 'second_party', 'facts']\n",
    "\n",
    "# \\b : 단어 경계, W* : 길이가 0이상이고 단어가 아닌 문자, w{1} : 길이가 1인 단어\n",
    "short_word = re.compile(r'\\W*\\b\\w{1}\\b') # 길이가 1인 단어 찾기\n",
    "tokenizer = TreebankWordTokenizer() # 단어 단위로 토큰화\n",
    "stopword = stopwords.words('english') # 불용어 리스트 가져오기\n",
    "lemmatizer = WordNetLemmatizer() # 단어의 기본 형태 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12a1fe9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:39:55.554256Z",
     "start_time": "2024-03-17T05:39:55.552232Z"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range = (1, 2)) # 출현빈도\n",
    "vec_facts = TfidfVectorizer(ngram_range = (1, 2)) # 단어토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cab20",
   "metadata": {},
   "source": [
    "## 단어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229dfb19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:39:55.561859Z",
     "start_time": "2024-03-17T05:39:55.555252Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepro1(df, cols, short_word, tokenizer, stopword, lemmatizer):\n",
    "    first_party_lst = []\n",
    "    second_party_lst = []\n",
    "    facts_lst = []\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].str.strip() # 공백 제거\n",
    "        df[col] = df[col].str.lower() # 소문자로 변경\n",
    "        df[col] = df[col].str.replace(',', '')\n",
    "        df[col] = df[col].str.replace('.', '')\n",
    "        \n",
    "        if col == 'first_party':\n",
    "            for content in df[col]:\n",
    "                content = short_word.sub('', content) # 한 글자 단어 제거\n",
    "                com = re.compile(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\") # 한글, 영어, 숫자 및 공백 문자를 제외한 모든 문자를 매칭\n",
    "                content = com.sub('', content)\n",
    "                tokens = tokenizer.tokenize(content) # 단어 토큰화\n",
    "                token_lst = []\n",
    "                for token in tokens:\n",
    "                    if token not in stopword: #불용어 제거\n",
    "                        token_lst.append(lemmatizer.lemmatize(token, 'n')) # 단어의 기본 형태 가져오기\n",
    "                first_party_lst.append(token_lst)\n",
    "            # 단어들 결합\n",
    "            for i in range(len(first_party_lst)):\n",
    "                first_party_lst[i] = ' '.join(first_party_lst[i])\n",
    "                \n",
    "        elif col == 'second_party':\n",
    "            for content in df[col]:\n",
    "                content = short_word.sub('', content) # 한 글자 단어 제거\n",
    "                com = re.compile(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\") # 한글, 영어, 숫자 및 공백 문자를 제외한 모든 문자를 매칭\n",
    "                content = com.sub('', content)\n",
    "                tokens = tokenizer.tokenize(content) # 단어 토큰화\n",
    "                token_lst = []\n",
    "                for token in tokens:\n",
    "                    if token not in stopword: #불용어 제거\n",
    "                        token_lst.append(lemmatizer.lemmatize(token, 'n')) # 단어의 기본 형태 가져오기\n",
    "                second_party_lst.append(token_lst)\n",
    "            # 단어들 결합\n",
    "            for i in range(len(second_party_lst)):\n",
    "                second_party_lst[i] = ' '.join(second_party_lst[i])\n",
    "                \n",
    "        elif col == 'facts':\n",
    "            for content in df[col]:\n",
    "                content = short_word.sub('', content) # 한 글자 단어 제거\n",
    "                com = re.compile(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\") # 한글, 영어, 숫자 및 공백 문자를 제외한 모든 문자를 매칭\n",
    "                content = com.sub('', content)\n",
    "                tokens = tokenizer.tokenize(content) # 단어 토큰화\n",
    "                token_lst = []\n",
    "                for token in tokens:\n",
    "                    if token not in stopword: #불용어 제거\n",
    "                        token_lst.append(lemmatizer.lemmatize(token, 'n')) # 단어의 기본 형태 가져오기\n",
    "                facts_lst.append(token_lst)\n",
    "            # 단어들 결합\n",
    "            for i in range(len(facts_lst)):\n",
    "                facts_lst[i] = ' '.join(facts_lst[i])\n",
    "                \n",
    "    return first_party_lst, second_party_lst, facts_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fdbaff",
   "metadata": {},
   "source": [
    "## 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52331c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:39:55.566125Z",
     "start_time": "2024-03-17T05:39:55.562852Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepro2(first, second, facts, vec, vec_facts, is_train):\n",
    "    if is_train:\n",
    "        vec.fit(first + second) # conut\n",
    "        vec_facts.fit(facts) # Tf\n",
    "    \n",
    "    X_first = vec.transform(first).toarray()\n",
    "    X_second = vec.transform(first).toarray()\n",
    "    X_facts = vec_facts.transform(facts).toarray()\n",
    "    \n",
    "    return np.concatenate([X_first, X_second, X_facts], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb8efb",
   "metadata": {},
   "source": [
    "전처리 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9711d45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:39:59.866867Z",
     "start_time": "2024-03-17T05:39:55.567147Z"
    }
   },
   "outputs": [],
   "source": [
    "train_first, train_second, train_facts = prepro1(train_df, cat_cols, short_word, tokenizer, stopword, lemmatizer)\n",
    "test_first, test_second, test_facts = prepro1(test_df, cat_cols, short_word, tokenizer, stopword, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06a68025",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:40:03.988294Z",
     "start_time": "2024-03-17T05:39:59.867894Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = prepro2(train_first, train_second, train_facts, vec, vec_facts, True)\n",
    "y_train = train_df['first_party_winner']\n",
    "X_test = prepro2(test_first, test_second, test_facts, vec, vec_facts, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "778097f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:40:03.993081Z",
     "start_time": "2024-03-17T05:40:03.990287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : (2478, 204261), (2478,)\n",
      "Test shape : (1240, 204261)\n"
     ]
    }
   ],
   "source": [
    "print('Train shape : {}, {}'.format(X_train.shape, y_train.shape))\n",
    "print('Test shape : {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e054c404",
   "metadata": {},
   "source": [
    "## 불균형 데이터 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a828be50",
   "metadata": {},
   "source": [
    "SMOTE\n",
    "\n",
    "- 소수 클래스의 샘플을 합성하여 데이터셋을 오버 샘플링하는 방법으로, 데이터 불균형 문제를 해결하는 데 널리 사용되는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dca5158e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:40:25.222517Z",
     "start_time": "2024-03-17T05:40:03.994077Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_neighbourhood_cleaning_rule.py:201: FutureWarning: `kind_sel` is deprecated in 0.12 and will be removed in 0.14. It already has not effect and corresponds to the `'all'` option.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape : (1775, 204261), (1775,)\n"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = smote = SMOTE(k_neighbors=5).fit_resample(X_train, y_train)\n",
    "print('train shape : {}, {}'.format(X_smote.shape, y_smote.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfc56643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:40:25.246530Z",
     "start_time": "2024-03-17T05:40:25.225546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "1    946\n",
       "0    829\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_smote.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4ae1e",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a9f6d",
   "metadata": {},
   "source": [
    "## 데이터 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e2ea2",
   "metadata": {},
   "source": [
    "train데이터 셋으로 train, val 데이터 셋으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e8b7954",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:40:27.213027Z",
     "start_time": "2024-03-17T05:40:25.247526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : (1420, 204261), (1420,)\n",
      "first_party_winner\n",
      "1    757\n",
      "0    663\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation shape : (355, 204261), (355,)\n",
      "first_party_winner\n",
      "1    189\n",
      "0    166\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_smote, y_smote, random_state = 42, test_size = 0.2, \n",
    "                                                 stratify = y_smote)\n",
    "\n",
    "print('Train shape : {}, {}'.format(X_train.shape, y_train.shape))\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print('Validation shape : {}, {}'.format(X_val.shape, y_val.shape))\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d3f70c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:39:43.278261Z",
     "start_time": "2024-03-17T05:39:43.275884Z"
    }
   },
   "outputs": [],
   "source": [
    "model_lst = ['Logistic Regression', 'KNN Classifier', 'Decision Tree', 'Random Forest', 'Ridge Claffifier',\n",
    "            'XGBoost', 'AdaBoost', 'Catboost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e0ef6ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:41:33.377944Z",
     "start_time": "2024-03-17T05:41:33.365479Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [ LogisticRegression(max_iter = 1000),\n",
    "                   KNeighborsClassifier(n_neighbors = 149, n_jobs = -1),\n",
    "                   DecisionTreeClassifier(),\n",
    "                   RandomForestClassifier(n_estimators = 100),\n",
    "                   RidgeClassifier(),\n",
    "                   XGBClassifier(),\n",
    "                   AdaBoostClassifier(),\n",
    "                   CatBoostClassifier(verbose = 0, random_state = 113)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecb971f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T05:47:06.284769Z",
     "start_time": "2024-03-17T05:47:06.281244Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(X_train, y_train, X_val, y_val, names, models):\n",
    "    score_df = pd.DataFrame()\n",
    "    score_train_lst = []\n",
    "    score_val_lst = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        score_train_lst.append(accuracy_score(y_train, y_train_pred))\n",
    "        score_val_lst.append(accuracy_score(y_val, y_val_pred))\n",
    "        \n",
    "    score_df['model'] = names\n",
    "    score_df['Train accuracy'] = score_train_lst\n",
    "    score_df['Validation accuracy'] = score_val_lst\n",
    "    score_df = score_df.sort_values('Validation accuracy', ascending = False)\n",
    "    return score_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4d0e8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-17T05:47:09.410Z"
    }
   },
   "outputs": [],
   "source": [
    "score(X_train, y_train, X_val, y_val, model_lst, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd2f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f184169b",
   "metadata": {},
   "source": [
    "# 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred  = best_logistic.predict(X_test)\n",
    "submission_df['first_party_winner'] = y_pred\n",
    "# submission_df.to_csv('neighbourhoodcleaningrule_logi.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636fe0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
