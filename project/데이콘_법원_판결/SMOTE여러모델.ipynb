{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca94ce9d",
   "metadata": {},
   "source": [
    "# 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77213fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:43.323760Z",
     "start_time": "2024-03-18T06:24:43.320899Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install -U imbalanced-learn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed4f45d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:43.327224Z",
     "start_time": "2024-03-18T06:24:43.324760Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814f548f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:45.508622Z",
     "start_time": "2024-03-18T06:24:43.329220Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer # 표준 토큰화\n",
    "from nltk.corpus import stopwords # 불용어 제거\n",
    "from nltk.stem import WordNetLemmatizer # 기본 형태로 변환\n",
    "from imblearn.over_sampling import SMOTE # 비대칭이라 사용\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f21329",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb5a900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:45.561710Z",
     "start_time": "2024-03-18T06:24:45.508890Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "submission_df = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "933ebd82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:45.575946Z",
     "start_time": "2024-03-18T06:24:45.562710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID         first_party                    second_party  \\\n",
       "0  TRAIN_0000   Phil A. St. Amant              Herman A. Thompson   \n",
       "1  TRAIN_0001      Stephen Duncan                  Lawrence Owens   \n",
       "2  TRAIN_0002   Billy Joe Magwood  Tony Patterson, Warden, et al.   \n",
       "3  TRAIN_0003          Linkletter                          Walker   \n",
       "4  TRAIN_0004  William Earl Fikes                         Alabama   \n",
       "\n",
       "                                               facts  first_party_winner  \n",
       "0  On June 27, 1962, Phil St. Amant, a candidate ...                   1  \n",
       "1  Ramon Nelson was riding his bike when he suffe...                   0  \n",
       "2  An Alabama state court convicted Billy Joe Mag...                   1  \n",
       "3  Victor Linkletter was convicted in state court...                   0  \n",
       "4  On April 24, 1953 in Selma, Alabama, an intrud...                   1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>Salerno</td>\n",
       "      <td>United States</td>\n",
       "      <td>The 1984 Bail Reform Act allowed the federal c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>Milberg Weiss Bershad Hynes and Lerach</td>\n",
       "      <td>Lexecon, Inc.</td>\n",
       "      <td>Lexecon Inc. was a defendant in a class action...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>No. 07-582\\t Title: \\t Federal Communications ...</td>\n",
       "      <td>Fox Television Stations, Inc., et al.</td>\n",
       "      <td>In 2002 and 2003, Fox Television Stations broa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>Harold Kaufman</td>\n",
       "      <td>United States</td>\n",
       "      <td>During his trial for armed robbery of a federa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>Berger</td>\n",
       "      <td>Hanlon</td>\n",
       "      <td>In 1993, a magistrate judge issued a warrant a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                        first_party  \\\n",
       "0  TEST_0000                                            Salerno   \n",
       "1  TEST_0001             Milberg Weiss Bershad Hynes and Lerach   \n",
       "2  TEST_0002  No. 07-582\\t Title: \\t Federal Communications ...   \n",
       "3  TEST_0003                                    Harold Kaufman    \n",
       "4  TEST_0004                                             Berger   \n",
       "\n",
       "                            second_party  \\\n",
       "0                          United States   \n",
       "1                          Lexecon, Inc.   \n",
       "2  Fox Television Stations, Inc., et al.   \n",
       "3                          United States   \n",
       "4                                 Hanlon   \n",
       "\n",
       "                                               facts  \n",
       "0  The 1984 Bail Reform Act allowed the federal c...  \n",
       "1  Lexecon Inc. was a defendant in a class action...  \n",
       "2  In 2002 and 2003, Fox Television Stations broa...  \n",
       "3  During his trial for armed robbery of a federa...  \n",
       "4  In 1993, a magistrate judge issued a warrant a...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0171c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:45.581977Z",
     "start_time": "2024-03-18T06:24:45.576947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "0     829\n",
       "1    1649\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('first_party_winner').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce13b7f",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff2411c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:45.587509Z",
     "start_time": "2024-03-18T06:24:45.582976Z"
    }
   },
   "outputs": [],
   "source": [
    "# 문자 처리\n",
    "cat_cols = ['first_party', 'second_party', 'facts']\n",
    "\n",
    "# \\b : 단어 경계, W* : 길이가 0이상이고 단어가 아닌 문자, w{1} : 길이가 1인 단어\n",
    "short_word = re.compile(r'\\W*\\b\\w{1}\\b') # 길이가 1인 단어 찾기\n",
    "tokenizer = TreebankWordTokenizer() # 단어 단위로 토큰화\n",
    "stopword = stopwords.words('english') # 불용어 리스트 가져오기\n",
    "lemmatizer = WordNetLemmatizer() # 단어의 기본 형태 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a1fe9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:45.591089Z",
     "start_time": "2024-03-18T06:24:45.588509Z"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range = (1, 2)) # 출현빈도\n",
    "vec_facts = TfidfVectorizer(ngram_range = (1, 2)) # 단어토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cab20",
   "metadata": {},
   "source": [
    "## 단어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "229dfb19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:45.601808Z",
     "start_time": "2024-03-18T06:24:45.593086Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepro1(df, cols, short_word, tokenizer, stopword, lemmatizer):\n",
    "    first_party_lst = []\n",
    "    second_party_lst = []\n",
    "    facts_lst = []\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].str.strip() # 공백 제거\n",
    "        df[col] = df[col].str.lower() # 소문자로 변경\n",
    "        df[col] = df[col].str.replace(',', '')\n",
    "        df[col] = df[col].str.replace('.', '')\n",
    "        \n",
    "        if col == 'first_party':\n",
    "            for content in df[col]:\n",
    "                content = short_word.sub('', content) # 한 글자 단어 제거\n",
    "                com = re.compile(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\") # 한글, 영어, 숫자 및 공백 문자를 제외한 모든 문자를 매칭\n",
    "                content = com.sub('', content)\n",
    "                tokens = tokenizer.tokenize(content) # 단어 토큰화\n",
    "                token_lst = []\n",
    "                for token in tokens:\n",
    "                    if token not in stopword: #불용어 제거\n",
    "                        token_lst.append(lemmatizer.lemmatize(token, 'n')) # 단어의 기본 형태 가져오기\n",
    "                first_party_lst.append(token_lst)\n",
    "            # 단어들 결합\n",
    "            for i in range(len(first_party_lst)):\n",
    "                first_party_lst[i] = ' '.join(first_party_lst[i])\n",
    "                \n",
    "        elif col == 'second_party':\n",
    "            for content in df[col]:\n",
    "                content = short_word.sub('', content) # 한 글자 단어 제거\n",
    "                com = re.compile(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\") # 한글, 영어, 숫자 및 공백 문자를 제외한 모든 문자를 매칭\n",
    "                content = com.sub('', content)\n",
    "                tokens = tokenizer.tokenize(content) # 단어 토큰화\n",
    "                token_lst = []\n",
    "                for token in tokens:\n",
    "                    if token not in stopword: #불용어 제거\n",
    "                        token_lst.append(lemmatizer.lemmatize(token, 'n')) # 단어의 기본 형태 가져오기\n",
    "                second_party_lst.append(token_lst)\n",
    "            # 단어들 결합\n",
    "            for i in range(len(second_party_lst)):\n",
    "                second_party_lst[i] = ' '.join(second_party_lst[i])\n",
    "                \n",
    "        elif col == 'facts':\n",
    "            for content in df[col]:\n",
    "                content = short_word.sub('', content) # 한 글자 단어 제거\n",
    "                com = re.compile(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\") # 한글, 영어, 숫자 및 공백 문자를 제외한 모든 문자를 매칭\n",
    "                content = com.sub('', content)\n",
    "                tokens = tokenizer.tokenize(content) # 단어 토큰화\n",
    "                token_lst = []\n",
    "                for token in tokens:\n",
    "                    if token not in stopword: #불용어 제거\n",
    "                        token_lst.append(lemmatizer.lemmatize(token, 'n')) # 단어의 기본 형태 가져오기\n",
    "                facts_lst.append(token_lst)\n",
    "            # 단어들 결합\n",
    "            for i in range(len(facts_lst)):\n",
    "                facts_lst[i] = ' '.join(facts_lst[i])\n",
    "                \n",
    "    return first_party_lst, second_party_lst, facts_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fdbaff",
   "metadata": {},
   "source": [
    "## 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52331c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:45.607512Z",
     "start_time": "2024-03-18T06:24:45.602809Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepro2(first, second, facts, vec, vec_facts, is_train):\n",
    "    if is_train:\n",
    "        vec.fit(first + second) # conut\n",
    "        vec_facts.fit(facts) # Tf\n",
    "    \n",
    "    X_first = vec.transform(first).toarray()\n",
    "    X_second = vec.transform(first).toarray()\n",
    "    X_facts = vec_facts.transform(facts).toarray()\n",
    "    \n",
    "    return np.concatenate([X_first, X_second, X_facts], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb8efb",
   "metadata": {},
   "source": [
    "전처리 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9711d45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:50.923195Z",
     "start_time": "2024-03-18T06:24:45.608513Z"
    }
   },
   "outputs": [],
   "source": [
    "train_first, train_second, train_facts = prepro1(train_df, cat_cols, short_word, tokenizer, stopword, lemmatizer)\n",
    "test_first, test_second, test_facts = prepro1(test_df, cat_cols, short_word, tokenizer, stopword, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06a68025",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:55.805301Z",
     "start_time": "2024-03-18T06:24:50.924603Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = prepro2(train_first, train_second, train_facts, vec, vec_facts, True)\n",
    "y_train = train_df['first_party_winner']\n",
    "X_test = prepro2(test_first, test_second, test_facts, vec, vec_facts, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "778097f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:24:55.810517Z",
     "start_time": "2024-03-18T06:24:55.806300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : (2478, 204261), (2478,)\n",
      "Test shape : (1240, 204261)\n"
     ]
    }
   ],
   "source": [
    "print('Train shape : {}, {}'.format(X_train.shape, y_train.shape))\n",
    "print('Test shape : {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e054c404",
   "metadata": {},
   "source": [
    "## 불균형 데이터 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a828be50",
   "metadata": {},
   "source": [
    "SMOTE\n",
    "\n",
    "- 소수 클래스의 샘플을 합성하여 데이터셋을 오버 샘플링하는 방법으로, 데이터 불균형 문제를 해결하는 데 널리 사용되는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dca5158e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:25:17.904897Z",
     "start_time": "2024-03-18T06:24:55.811518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape : (3298, 204261), (3298,)\n"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = smote = SMOTE(k_neighbors=5).fit_resample(X_train, y_train)\n",
    "print('train shape : {}, {}'.format(X_smote.shape, y_smote.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfc56643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:25:18.038116Z",
     "start_time": "2024-03-18T06:25:17.921854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "1    1649\n",
       "0    1649\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_smote.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4ae1e",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a9f6d",
   "metadata": {},
   "source": [
    "## 데이터 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e2ea2",
   "metadata": {},
   "source": [
    "train데이터 셋으로 train, val 데이터 셋으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e8b7954",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:25:19.915943Z",
     "start_time": "2024-03-18T06:25:18.044103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : (2638, 204261), (2638,)\n",
      "first_party_winner\n",
      "1    1319\n",
      "0    1319\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation shape : (660, 204261), (660,)\n",
      "first_party_winner\n",
      "0    330\n",
      "1    330\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_smote, y_smote, random_state = 42, test_size = 0.2, \n",
    "                                                 stratify = y_smote)\n",
    "\n",
    "print('Train shape : {}, {}'.format(X_train.shape, y_train.shape))\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print('Validation shape : {}, {}'.format(X_val.shape, y_val.shape))\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d3f70c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:25:19.924199Z",
     "start_time": "2024-03-18T06:25:19.917940Z"
    }
   },
   "outputs": [],
   "source": [
    "model_lst = ['Logistic Regression', 'KNN Classifier', 'Decision Tree', 'Random Forest', 'Ridge Claffifier',\n",
    "            'XGBoost', 'AdaBoost', 'Catboost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e0ef6ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:25:19.972247Z",
     "start_time": "2024-03-18T06:25:19.927193Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [ LogisticRegression(max_iter = 500, random_state = 123),\n",
    "                   KNeighborsClassifier(n_neighbors = 149, n_jobs = -1),\n",
    "                   DecisionTreeClassifier(),\n",
    "                   RandomForestClassifier(n_estimators = 100),\n",
    "                   RidgeClassifier(),\n",
    "                   XGBClassifier(),\n",
    "                   AdaBoostClassifier(),\n",
    "                   CatBoostClassifier(verbose = 0, random_state = 113)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecb971f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:25:19.991560Z",
     "start_time": "2024-03-18T06:25:19.977237Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(X_train, y_train, X_val, y_val, names, models):\n",
    "    score_df = pd.DataFrame()\n",
    "    score_train_lst = []\n",
    "    score_val_lst = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        score_train_lst.append(accuracy_score(y_train, y_train_pred))\n",
    "        score_val_lst.append(accuracy_score(y_val, y_val_pred))\n",
    "        \n",
    "    score_df['model'] = names\n",
    "    score_df['Train accuracy'] = score_train_lst\n",
    "    score_df['Validation accuracy'] = score_val_lst\n",
    "    score_df = score_df.sort_values('Validation accuracy', ascending = False)\n",
    "    return score_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54c4d0e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:57:36.938212Z",
     "start_time": "2024-03-18T06:25:19.993556Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mit005\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Validation accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.824242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ridge Claffifier</td>\n",
       "      <td>0.999621</td>\n",
       "      <td>0.821212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.990902</td>\n",
       "      <td>0.809091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.995072</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.965883</td>\n",
       "      <td>0.765152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.703030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.757013</td>\n",
       "      <td>0.684848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>0.509856</td>\n",
       "      <td>0.518182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  Train accuracy  Validation accuracy\n",
       "3        Random Forest        1.000000             0.824242\n",
       "4     Ridge Claffifier        0.999621             0.821212\n",
       "0  Logistic Regression        0.990902             0.809091\n",
       "5              XGBoost        0.995072             0.766667\n",
       "7             Catboost        0.965883             0.765152\n",
       "2        Decision Tree        1.000000             0.703030\n",
       "6             AdaBoost        0.757013             0.684848\n",
       "1       KNN Classifier        0.509856             0.518182"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(X_train, y_train, X_val, y_val, model_lst, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd2f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f184169b",
   "metadata": {},
   "source": [
    "# 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09a3747c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T06:57:38.717406Z",
     "start_time": "2024-03-18T06:57:36.963675Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_logistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred  \u001b[38;5;241m=\u001b[39m best_logistic\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      2\u001b[0m submission_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_party_winner\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_pred\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_logistic' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred  = best_logistic.predict(X_test)\n",
    "submission_df['first_party_winner'] = y_pred\n",
    "# submission_df.to_csv('neighbourhoodcleaningrule_logi.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636fe0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
