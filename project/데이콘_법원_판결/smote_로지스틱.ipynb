{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca94ce9d",
   "metadata": {},
   "source": [
    "# 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77213fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:03:37.718196Z",
     "start_time": "2024-03-19T07:03:37.715822Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install -U imbalanced-learn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "814f548f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:03:41.608742Z",
     "start_time": "2024-03-19T07:03:37.719201Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer # 표준 토큰화\n",
    "from nltk.corpus import stopwords # 불용어 제거\n",
    "from nltk.stem import WordNetLemmatizer # 기본 형태로 변환\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f21329",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb5a900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:03:41.680356Z",
     "start_time": "2024-03-19T07:03:41.611268Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "submission_df = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "933ebd82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:03:41.699559Z",
     "start_time": "2024-03-19T07:03:41.681859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID         first_party                    second_party  \\\n",
       "0  TRAIN_0000   Phil A. St. Amant              Herman A. Thompson   \n",
       "1  TRAIN_0001      Stephen Duncan                  Lawrence Owens   \n",
       "2  TRAIN_0002   Billy Joe Magwood  Tony Patterson, Warden, et al.   \n",
       "3  TRAIN_0003          Linkletter                          Walker   \n",
       "4  TRAIN_0004  William Earl Fikes                         Alabama   \n",
       "\n",
       "                                               facts  first_party_winner  \n",
       "0  On June 27, 1962, Phil St. Amant, a candidate ...                   1  \n",
       "1  Ramon Nelson was riding his bike when he suffe...                   0  \n",
       "2  An Alabama state court convicted Billy Joe Mag...                   1  \n",
       "3  Victor Linkletter was convicted in state court...                   0  \n",
       "4  On April 24, 1953 in Selma, Alabama, an intrud...                   1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>Salerno</td>\n",
       "      <td>United States</td>\n",
       "      <td>The 1984 Bail Reform Act allowed the federal c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>Milberg Weiss Bershad Hynes and Lerach</td>\n",
       "      <td>Lexecon, Inc.</td>\n",
       "      <td>Lexecon Inc. was a defendant in a class action...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>No. 07-582\\t Title: \\t Federal Communications ...</td>\n",
       "      <td>Fox Television Stations, Inc., et al.</td>\n",
       "      <td>In 2002 and 2003, Fox Television Stations broa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>Harold Kaufman</td>\n",
       "      <td>United States</td>\n",
       "      <td>During his trial for armed robbery of a federa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>Berger</td>\n",
       "      <td>Hanlon</td>\n",
       "      <td>In 1993, a magistrate judge issued a warrant a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                        first_party  \\\n",
       "0  TEST_0000                                            Salerno   \n",
       "1  TEST_0001             Milberg Weiss Bershad Hynes and Lerach   \n",
       "2  TEST_0002  No. 07-582\\t Title: \\t Federal Communications ...   \n",
       "3  TEST_0003                                    Harold Kaufman    \n",
       "4  TEST_0004                                             Berger   \n",
       "\n",
       "                            second_party  \\\n",
       "0                          United States   \n",
       "1                          Lexecon, Inc.   \n",
       "2  Fox Television Stations, Inc., et al.   \n",
       "3                          United States   \n",
       "4                                 Hanlon   \n",
       "\n",
       "                                               facts  \n",
       "0  The 1984 Bail Reform Act allowed the federal c...  \n",
       "1  Lexecon Inc. was a defendant in a class action...  \n",
       "2  In 2002 and 2003, Fox Television Stations broa...  \n",
       "3  During his trial for armed robbery of a federa...  \n",
       "4  In 1993, a magistrate judge issued a warrant a...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce88281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:03:41.707396Z",
     "start_time": "2024-03-19T07:03:41.701062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "0     829\n",
       "1    1649\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('first_party_winner').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce13b7f",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff2411c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:03:41.715987Z",
     "start_time": "2024-03-19T07:03:41.708403Z"
    }
   },
   "outputs": [],
   "source": [
    "# 문자 처리\n",
    "cat_cols = ['first_party', 'second_party', 'facts']\n",
    "\n",
    "# \\b : 단어 경계, W* : 길이가 0이상이고 단어가 아닌 문자, w{1} : 길이가 1인 단어\n",
    "short_word = re.compile(r'\\W*\\b\\w{1}\\b') # 길이가 1인 단어 찾기\n",
    "tokenizer = TreebankWordTokenizer() # 단어 단위로 토큰화\n",
    "stopword = stopwords.words('english') # 불용어 리스트 가져오기\n",
    "lemmatizer = WordNetLemmatizer() # 단어의 기본 형태 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6938affb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:03:41.721615Z",
     "start_time": "2024-03-19T07:03:41.717991Z"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range = (1, 2)) # 출현빈도\n",
    "vec_facts = TfidfVectorizer(ngram_range = (1, 2)) # 단어토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dce8ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:03:41.732456Z",
     "start_time": "2024-03-19T07:03:41.723616Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepro1(df, cols, short_word, tokenizer, stopword, lemmatizer):\n",
    "    first_party_lst = []\n",
    "    second_party_lst = []\n",
    "    facts_lst = []\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].str.strip() # 공백 제거\n",
    "        df[col] = df[col].str.lower() # 소문자로 변경\n",
    "        df[col] = df[col].str.replace(',', '')\n",
    "        df[col] = df[col].str.replace('.', '')\n",
    "        \n",
    "        if col == 'first_party':\n",
    "            for content in df[col]:\n",
    "                content = short_word.sub('', content) # 한 글자 단어 제거\n",
    "                com = re.compile(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\") # 한글, 영어, 숫자 및 공백 문자를 제외한 모든 문자를 매칭\n",
    "                content = com.sub('', content)\n",
    "                tokens = tokenizer.tokenize(content) # 단어 토큰화\n",
    "                token_lst = []\n",
    "                for token in tokens:\n",
    "                    if token not in stopword: #불용어 제거\n",
    "                        token_lst.append(lemmatizer.lemmatize(token, 'n')) # 단어의 기본 형태 가져오기\n",
    "                first_party_lst.append(token_lst)\n",
    "            # 단어들 결합\n",
    "            for i in range(len(first_party_lst)):\n",
    "                first_party_lst[i] = ' '.join(first_party_lst[i])\n",
    "                \n",
    "        elif col == 'second_party':\n",
    "            for content in df[col]:\n",
    "                content = short_word.sub('', content) # 한 글자 단어 제거\n",
    "                com = re.compile(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\") # 한글, 영어, 숫자 및 공백 문자를 제외한 모든 문자를 매칭\n",
    "                content = com.sub('', content)\n",
    "                tokens = tokenizer.tokenize(content) # 단어 토큰화\n",
    "                token_lst = []\n",
    "                for token in tokens:\n",
    "                    if token not in stopword: #불용어 제거\n",
    "                        token_lst.append(lemmatizer.lemmatize(token, 'n')) # 단어의 기본 형태 가져오기\n",
    "                second_party_lst.append(token_lst)\n",
    "            # 단어들 결합\n",
    "            for i in range(len(second_party_lst)):\n",
    "                second_party_lst[i] = ' '.join(second_party_lst[i])\n",
    "                \n",
    "        elif col == 'facts':\n",
    "            for content in df[col]:\n",
    "                content = short_word.sub('', content) # 한 글자 단어 제거\n",
    "                com = re.compile(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\") # 한글, 영어, 숫자 및 공백 문자를 제외한 모든 문자를 매칭\n",
    "                content = com.sub('', content)\n",
    "                tokens = tokenizer.tokenize(content) # 단어 토큰화\n",
    "                token_lst = []\n",
    "                for token in tokens:\n",
    "                    if token not in stopword: #불용어 제거\n",
    "                        token_lst.append(lemmatizer.lemmatize(token, 'n')) # 단어의 기본 형태 가져오기\n",
    "                facts_lst.append(token_lst)\n",
    "            # 단어들 결합\n",
    "            for i in range(len(facts_lst)):\n",
    "                facts_lst[i] = ' '.join(facts_lst[i])\n",
    "                \n",
    "    return first_party_lst, second_party_lst, facts_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8468f14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:03:41.739633Z",
     "start_time": "2024-03-19T07:03:41.735457Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepro2(first, second, facts, vec, vec_facts, is_train):\n",
    "    if is_train:\n",
    "        vec.fit(first + second) # conut\n",
    "        vec_facts.fit(facts) # Tf\n",
    "    \n",
    "    X_first = vec.transform(first).toarray()\n",
    "    X_second = vec.transform(first).toarray()\n",
    "    X_facts = vec_facts.transform(facts).toarray()\n",
    "    \n",
    "    return np.concatenate([X_first, X_second, X_facts], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fc4c92b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:03:49.081038Z",
     "start_time": "2024-03-19T07:03:41.741137Z"
    }
   },
   "outputs": [],
   "source": [
    "train_first, train_second, train_facts = prepro1(train_df, cat_cols, short_word, tokenizer, stopword, lemmatizer)\n",
    "test_first, test_second, test_facts = prepro1(test_df, cat_cols, short_word, tokenizer, stopword, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b00114a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:01.800274Z",
     "start_time": "2024-03-19T07:03:49.082041Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = prepro2(train_first, train_second, train_facts, vec, vec_facts, True)\n",
    "y_train = train_df['first_party_winner']\n",
    "X_test = prepro2(test_first, test_second, test_facts, vec, vec_facts, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c4a1ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:01.844298Z",
     "start_time": "2024-03-19T07:04:01.817267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : (2478, 204261), (2478,)\n",
      "Test shape : (1240, 204261)\n"
     ]
    }
   ],
   "source": [
    "print('Train shape : {}, {}'.format(X_train.shape, y_train.shape))\n",
    "print('Test shape : {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e054c404",
   "metadata": {},
   "source": [
    "## 불균형 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca5158e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:25.378716Z",
     "start_time": "2024-03-19T07:04:01.846300Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.02 GiB for an array with shape (3298, 204261) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_smote, y_smote \u001b[38;5;241m=\u001b[39m SMOTE(k_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain shape : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(X_smote\u001b[38;5;241m.\u001b[39mshape, y_smote\u001b[38;5;241m.\u001b[39mshape))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[1;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n\u001b[0;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    118\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:392\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    390\u001b[0m     X_resampled \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mvstack(X_resampled, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mformat)\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 392\u001b[0m     X_resampled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(X_resampled)\n\u001b[0;32m    393\u001b[0m y_resampled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack(y_resampled)\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_resampled, y_resampled\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    295\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.02 GiB for an array with shape (3298, 204261) and data type float64"
     ]
    }
   ],
   "source": [
    "X_smote, y_smote = SMOTE(k_neighbors=5).fit_resample(X_train, y_train)\n",
    "print('train shape : {}, {}'.format(X_smote.shape, y_smote.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc56643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:25.381242Z",
     "start_time": "2024-03-19T07:04:25.381242Z"
    }
   },
   "outputs": [],
   "source": [
    "y_smote.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b876c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:25.383238Z",
     "start_time": "2024-03-19T07:04:25.383238Z"
    }
   },
   "outputs": [],
   "source": [
    "## 패배/승리 데이터 불균형 확인\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "\n",
    "train_label_counts = y_smote.value_counts()\n",
    "\n",
    "# 색상 팔레트 생성\n",
    "palette = sns.color_palette(\"husl\", len(train_label_counts))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=train_label_counts.index, y=train_label_counts.values, palette=palette)\n",
    "plt.title('Train Label Distribution(SMOTE)')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(y_smote.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee94189",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62232246",
   "metadata": {},
   "source": [
    "## 데이터 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec92e12",
   "metadata": {},
   "source": [
    "train데이터 셋으로 train, val 데이터 셋으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7cb7cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:25.384235Z",
     "start_time": "2024-03-19T07:04:25.384235Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_smote, y_smote, random_state = 42, test_size = 0.2, \n",
    "                                                 stratify = y_smote)\n",
    "\n",
    "print('Train shape : {}, {}'.format(X_train.shape, y_train.shape))\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print('Validation shape : {}, {}'.format(X_val.shape, y_val.shape))\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d9a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:25.386230Z",
     "start_time": "2024-03-19T07:04:25.386230Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter = 500, random_state = 123)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a388ff6",
   "metadata": {},
   "source": [
    "=> macro avg는 클래스 별 성능 지표의 단순 평균을 계산\n",
    "\n",
    "=> weighted avg는 각 클래스의 지지도를 가중치로 사용하여 계산된 평균\n",
    "\n",
    "=> 모델이 양성 및 음성 클래스를 비교적 균형있게 예측하고 있으며, 적절한 성능을 보이고 있는 것으로 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be39d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:25.387227Z",
     "start_time": "2024-03-19T07:04:25.387227Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "print('train accuracy : ', accuracy_score(y_train, y_train_pred))\n",
    "print('validation accuracy : ', accuracy_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e3f16",
   "metadata": {},
   "source": [
    "# 그리드서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc377191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:25.389223Z",
     "start_time": "2024-03-19T07:04:25.389223Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = LogisticRegression(random_state = 42)\n",
    "\n",
    "# params = {'C' : [0.1, 1, 10, 100], 'max_iter' : [300, 400, 500]}\n",
    "# grid = GridSearchCV(model, param_grid = params, cv = 5 )\n",
    "\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2697357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:25.390725Z",
     "start_time": "2024-03-19T07:04:25.390725Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 최적의 하이퍼파라미터로 로지스틱 회귀 모델 생성\n",
    "# best_logistic = LogisticRegression(**best_params)\n",
    "\n",
    "# # 모델을 훈련 데이터에 적합\n",
    "# best_logistic.fit(X_train, y_train)\n",
    "\n",
    "# # 테스트 데이터에 모델을 적용하여 예측 수행\n",
    "# pred = best_logistic.predict(X_val)\n",
    "# print(classification_report(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9029e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d70e0ff",
   "metadata": {},
   "source": [
    "# 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ba43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:25.391724Z",
     "start_time": "2024-03-19T07:04:25.391724Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "submission_df['first_party_winner'] = y_pred\n",
    "submission_df.to_csv('smote_logit.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e716e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T07:04:25.393721Z",
     "start_time": "2024-03-19T07:04:25.393721Z"
    }
   },
   "outputs": [],
   "source": [
    "# y_pred  = best_logistic.predict(X_test)\n",
    "# submission_df['first_party_winner'] = y_pred\n",
    "# submission_df.to_csv('neighbourhoodcleaningrule_logit.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
